{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 3}, {'a': 5}]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0b20ce65a23c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "x = list()\n",
    "item1 = dict()\n",
    "item1[\"a\"] = 3\n",
    "item2 = dict()\n",
    "item2[\"a\"] = 5\n",
    "if item1 not in x:\n",
    "    x.append(item1)\n",
    "if item2 not in x:\n",
    "    x.append(item2)\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = [(['1 2', '2 3'], ['3 4']), (['1 2'], ['5 8'])]\n",
    "print( (['1 2'], ['5 8']) in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asd', 'sadasf', 'dasdaf']\n"
     ]
    }
   ],
   "source": [
    "x = dict()\n",
    "x['asd'] = 1\n",
    "x['sadasf'] = 2\n",
    "x['dasdaf'] = 3\n",
    "print(list(x.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos singular present 121546\n",
      "pos singular past 62363\n",
      "pos plural present 18791\n",
      "pos plural past 6041\n",
      "neg singular present 57558\n",
      "neg singular past 79752\n",
      "neg plural present 7218\n",
      "neg plural past 8618\n",
      "24000 24000\n"
     ]
    }
   ],
   "source": [
    "path = '/data2/home/zhaoyi/compctg/dataset/Yelp/Yelp.json'\n",
    "fp = open(path, \"r\")\n",
    "import json\n",
    "_data = json.load(fp)\n",
    "\n",
    "data = dict()\n",
    "for sent in ['pos', 'neg']:\n",
    "    data[sent] = dict()\n",
    "    for pronoun in ['singular', 'plural']:\n",
    "        data[sent][pronoun] = dict()\n",
    "        for tense in ['present', 'past']:\n",
    "            data[sent][pronoun][tense] = list()\n",
    "\n",
    "for datum in _data:\n",
    "    sent = datum[\"sentiment\"]\n",
    "    pronoun = datum[\"pronoun\"]\n",
    "    tense = datum[\"tense\"]\n",
    "    data[sent][pronoun][tense].append(datum)\n",
    "\n",
    "for sent in ['pos', 'neg']:\n",
    "    for pronoun in ['singular', 'plural']:\n",
    "        for tense in ['present', 'past']:\n",
    "            print(sent, pronoun, tense, len(data[sent][pronoun][tense]))\n",
    "\n",
    "# len = 6000\n",
    "min_length = 6000\n",
    "cls_path = \"/data2/home/zhaoyi/compctg/dataset/Yelp/cls.json\"\n",
    "gen_path = \"/data2/home/zhaoyi/compctg/dataset/Yelp/gen.json\"\n",
    "\n",
    "fw_cls = open(cls_path, \"w\")\n",
    "fw_gen = open(gen_path, \"w\")\n",
    "\n",
    "cnt = dict()\n",
    "for sent in ['pos', 'neg']:\n",
    "    cnt[sent] = dict()\n",
    "    for pronoun in ['singular', 'plural']:\n",
    "        cnt[sent][pronoun] = dict()\n",
    "        for tense in ['present', 'past']:\n",
    "            cnt[sent][pronoun][tense] = 0\n",
    "cls_data = list()\n",
    "gen_data = list()\n",
    "for datum in _data:\n",
    "    if cnt[datum[\"sentiment\"]][datum[\"pronoun\"]][datum[\"tense\"]] < 3000:\n",
    "        cls_data.append(datum)\n",
    "    elif cnt[datum[\"sentiment\"]][datum[\"pronoun\"]][datum[\"tense\"]] < 6000:\n",
    "        gen_data.append(datum)\n",
    "    cnt[datum[\"sentiment\"]][datum[\"pronoun\"]][datum[\"tense\"]] += 1\n",
    "print(len(cls_data), len(gen_data))\n",
    "json.dump(cls_data, fw_cls, indent=4)\n",
    "json.dump(gen_data, fw_gen, indent=4)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos books 25000\n",
      "pos clothing 25000\n",
      "pos music 25000\n",
      "pos electronics 25000\n",
      "pos movies 25000\n",
      "pos sports 25000\n",
      "neg books 25000\n",
      "neg clothing 25000\n",
      "neg music 25000\n",
      "neg electronics 25000\n",
      "neg movies 25000\n",
      "neg sports 25000\n",
      "180000 120000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(2023)\n",
    "path = '/data2/home/zhaoyi/compctg/dataset/Amazon-Review/amazon_review.json'\n",
    "fp = open(path, \"r\")\n",
    "import json\n",
    "_data = json.load(fp)\n",
    "\n",
    "data = dict()\n",
    "cnt = dict()\n",
    "for sent in ['pos', 'neg']:\n",
    "    data[sent] = dict()\n",
    "    cnt[sent] = dict()\n",
    "    for topic in [\"books\", \"clothing\", \"music\", \"electronics\", \"movies\", \"sports\"]:\n",
    "            data[sent][topic] = list()\n",
    "            cnt[sent][topic] = 0\n",
    "\n",
    "for datum in _data:\n",
    "    sent = datum[\"sent\"]\n",
    "    topic = datum[\"topic\"]\n",
    "    data[sent][topic].append(datum)\n",
    "\n",
    "for sent in ['pos', 'neg']:\n",
    "    for topic in [\"books\", \"clothing\", \"music\", \"electronics\", \"movies\", \"sports\"]:\n",
    "        random.shuffle(data[sent][topic])\n",
    "        print(sent, topic, len(data[sent][topic]))\n",
    "\n",
    "cls_data = list()\n",
    "gen_data = list()\n",
    "for datum in _data:\n",
    "    sent = datum[\"sent\"]\n",
    "    topic = datum[\"topic\"]\n",
    "    if cnt[sent][topic] < 10000:\n",
    "        gen_data.append(datum)\n",
    "    elif cnt[sent][topic] < 25000:\n",
    "        cls_data.append(datum)\n",
    "    cnt[sent][topic] += 1\n",
    "\n",
    "cls_path = '/data2/home/zhaoyi/compctg/dataset/Amazon-Review/cls.json'\n",
    "gen_path = '/data2/home/zhaoyi/compctg/dataset/Amazon-Review/gen.json'\n",
    "\n",
    "cls_fr = open(cls_path, \"w\")\n",
    "gen_fr = open(gen_path, \"w\")\n",
    "\n",
    "json.dump(cls_data, cls_fr, indent=4)\n",
    "json.dump(gen_data, gen_fr, indent=4)\n",
    "print(len(cls_data), len(gen_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 4] [5, 6, 7, 8, 8]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 2, 3, 4]\n",
    "y = [5, 6, 7, 8]\n",
    "def func(x, y):\n",
    "    x.append(4)\n",
    "    y.append(8)\n",
    "func(x, y)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"/data2/home/zhaoyi/compctg/aclImdb/train/pos\"\n",
    "pos = list()\n",
    "for root,folders,files in os.walk(path):\n",
    "    for file in files:\n",
    "        fr = open(path+'/'+file, \"r\")\n",
    "        lines = list(fr.readlines())\n",
    "        assert len(lines) == 1\n",
    "        pos.append(lines[0])\n",
    "        # print(file)\n",
    "\n",
    "path = \"/data2/home/zhaoyi/compctg/aclImdb/train/neg\"\n",
    "neg = list()\n",
    "for root,folders,files in os.walk(path):\n",
    "    for file in files:\n",
    "        fr = open(path+'/'+file, \"r\")\n",
    "        lines = list(fr.readlines())\n",
    "        assert len(lines) == 1\n",
    "        neg.append(lines[0])\n",
    "        # print(file)       \n",
    "\n",
    "pos_write_path = \"/data2/home/zhaoyi/compctg/aclImdb/train/pos.txt\"\n",
    "neg_write_path = \"/data2/home/zhaoyi/compctg/aclImdb/train/neg.txt\"\n",
    "fw_pos = open(pos_write_path, \"w\")\n",
    "fw_neg = open(neg_write_path, \"w\")\n",
    "for d in pos:\n",
    "    fw_pos.write(d+'\\n')\n",
    "for d in neg:\n",
    "    fw_neg.write(d+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12500\n",
      "12500\n",
      "2576\n",
      "1133\n",
      "2538\n",
      "1971\n",
      "3321\n",
      "3343\n",
      "imdb pos 12500\n",
      "imdb neg 12500\n",
      "opener pos 2576\n",
      "opener neg 1133\n",
      "auto pos 2538\n",
      "auto neg 1971\n",
      "tablets pos 3321\n",
      "tablets neg 3343\n",
      "4264 4800\n",
      "4264 4800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncls_path = \"/data2/home/zhaoyi/compctg/dataset/Fyelp-v4/cls.json\"\\ngen_path = \"/data2/home/zhaoyi/compctg/dataset/Fyelp-v4/gen.json\"\\n\\nfr_cls = open(cls_path, \"r\")\\nfr_gen = open(gen_path, \"r\")\\ncls_data = json.load(fr_cls)\\ngen_data = json.load(fr_gen)\\nprint(\\'here\\', len(cls_data), len(gen_data))\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "imdb_pos_path = \"/data2/home/zhaoyi/compctg/aclImdb/train/pos.txt\"\n",
    "imdb_neg_path = \"/data2/home/zhaoyi/compctg/aclImdb/train/neg.txt\"\n",
    "opener_pos_path = \"/data2/home/zhaoyi/compctg/sota_sentiment-master/datasets/opener/pos.txt\"\n",
    "opener_neg_path = \"/data2/home/zhaoyi/compctg/sota_sentiment-master/datasets/opener/neg.txt\"\n",
    "auto_pos_path = \"/data2/home/zhaoyi/compctg/sota_sentiment-master/datasets/SenTube/auto/pos.txt\"\n",
    "auto_neg_path = \"/data2/home/zhaoyi/compctg/sota_sentiment-master/datasets/SenTube/auto/neg.txt\"\n",
    "tablets_pos_path = \"/data2/home/zhaoyi/compctg/sota_sentiment-master/datasets/SenTube/tablets/pos.txt\"\n",
    "tablets_neg_path = \"/data2/home/zhaoyi/compctg/sota_sentiment-master/datasets/SenTube/tablets/neg.txt\"\n",
    "\n",
    "\n",
    "fw = dict()\n",
    "for topic in [\"imdb\", \"opener\", \"auto\", \"tablets\"]:\n",
    "    fw[topic] = dict()\n",
    "\n",
    "fw[\"imdb\"][\"pos\"] = open(imdb_pos_path,\"r\")\n",
    "#print(fw[\"imdb\"][\"pos\"])\n",
    "fw[\"imdb\"][\"neg\"] = open(imdb_neg_path,\"r\")\n",
    "fw[\"opener\"][\"pos\"] = open(opener_pos_path,\"r\")\n",
    "fw[\"opener\"][\"neg\"] = open(opener_neg_path,\"r\")\n",
    "fw[\"auto\"][\"pos\"] = open(auto_pos_path,\"r\")\n",
    "fw[\"auto\"][\"neg\"]= open(auto_neg_path,\"r\")\n",
    "fw[\"tablets\"][\"pos\"] = open(tablets_pos_path,\"r\")\n",
    "fw[\"tablets\"][\"neg\"] = open(tablets_neg_path,\"r\")\n",
    "\n",
    "data = list()\n",
    "cnt =dict()\n",
    "for topic in [\"imdb\", \"opener\", \"auto\", \"tablets\"]:\n",
    "    cnt[topic] = dict()\n",
    "    for sent in [\"pos\",\"neg\"]:\n",
    "        cnt[topic][sent] = 0\n",
    "\n",
    "for topic in [\"imdb\", \"opener\", \"auto\", \"tablets\"]:\n",
    "    for sent in [\"pos\",\"neg\"]:\n",
    "        lines = fw[topic][sent].readlines()\n",
    "        print(len(list(lines)))\n",
    "        for line in lines:\n",
    "            line = line.rstrip('\\n')\n",
    "            datum = dict()\n",
    "            datum[\"topic_cged\"] = topic\n",
    "            datum[\"sentiment\"] = sent\n",
    "            #datum[\"review\"] = line.replace(\"\\\"\",\"\\'\")\n",
    "            datum[\"review\"] = line\n",
    "            data.append(datum)\n",
    "            cnt[topic][sent] += 1\n",
    "\n",
    "for topic in [\"imdb\", \"opener\", \"auto\", \"tablets\"]:\n",
    "    for sent in [\"pos\",\"neg\"]:\n",
    "        print(topic, sent, cnt[topic][sent])\n",
    "\n",
    "length = 1133\n",
    "cls_data = list()\n",
    "gen_data = list()\n",
    "cls_cnt =dict()\n",
    "for topic in [\"imdb\", \"opener\", \"auto\", \"tablets\"]:\n",
    "    cls_cnt[topic] = dict()\n",
    "    for sent in [\"pos\",\"neg\"]:\n",
    "        cls_cnt[topic][sent] = 0\n",
    "\n",
    "for datum in data:\n",
    "    if cls_cnt[datum[\"topic_cged\"]][datum[\"sentiment\"]] < 533:\n",
    "        cls_data.append(datum)\n",
    "    elif cls_cnt[datum[\"topic_cged\"]][datum[\"sentiment\"]] < 1133:\n",
    "        gen_data.append(datum)\n",
    "    cls_cnt[datum[\"topic_cged\"]][datum[\"sentiment\"]] += 1\n",
    "\n",
    "print(len(cls_data), len(gen_data))\n",
    "'''\n",
    "cls_path = \"/data2/home/zhaoyi/compctg/dataset/Mixture/cls.txt\"\n",
    "gen_path = \"/data2/home/zhaoyi/compctg/dataset/Mixture/gen.txt\"\n",
    "'''\n",
    "cls_path = \"/data2/home/zhaoyi/compctg/dataset/Mixture/cls.json\"\n",
    "gen_path = \"/data2/home/zhaoyi/compctg/dataset/Mixture/gen.json\"\n",
    "fw_cls = open(cls_path, \"w\")\n",
    "fw_gen = open(gen_path, \"w\")\n",
    "\n",
    "import json\n",
    "import random\n",
    "random.shuffle(cls_data)\n",
    "random.shuffle(gen_data)\n",
    "print(len(cls_data), len(gen_data))\n",
    "'''\n",
    "for datum in cls_data:\n",
    "    fw_cls.write(datum[\"review\"]+'-----'+datum[\"sentiment\"]+'-----'+datum[\"topic_cged\"]+'\\n')\n",
    "\n",
    "for datum in gen_data:\n",
    "    fw_gen.write(datum[\"review\"]+'-----'+datum[\"sentiment\"]+'-----'+datum[\"topic_cged\"]+'\\n')\n",
    "'''\n",
    "'''\n",
    "for datum in cls_data:\n",
    "    json.dump(datum, fw_cls)\n",
    "    fw_cls.write('\\n')\n",
    "for datum in gen_data:\n",
    "    json.dump(datum, fw_gen)\n",
    "    fw_gen.write('\\n')   \n",
    "'''\n",
    "\n",
    "\n",
    "json.dump(cls_data, fw_cls, indent=4)\n",
    "json.dump(gen_data, fw_gen, indent=4)\n",
    "\n",
    "fw_gen.close()\n",
    "fw_cls.close()\n",
    "\"\"\"\n",
    "cls_path = \"/data2/home/zhaoyi/compctg/dataset/Fyelp-v4/cls.json\"\n",
    "gen_path = \"/data2/home/zhaoyi/compctg/dataset/Fyelp-v4/gen.json\"\n",
    "\n",
    "fr_cls = open(cls_path, \"r\")\n",
    "fr_gen = open(gen_path, \"r\")\n",
    "cls_data = json.load(fr_cls)\n",
    "gen_data = json.load(fr_gen)\n",
    "print('here', len(cls_data), len(gen_data))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sa'dasf'\n"
     ]
    }
   ],
   "source": [
    "x = \"\\\"sa\\\"dasf\\\"\"\n",
    "print(x.replace(\"\\\"\",\"\\'\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
